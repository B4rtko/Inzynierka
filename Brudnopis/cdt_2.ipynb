{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 17:39:07.916553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 17:39:08.035880: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-16 17:39:08.035906: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-16 17:39:08.878363: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 17:39:08.878490: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-16 17:39:08.878504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "         Data  Otwarcie  Najwyzszy  Najnizszy  Zamkniecie    Wolumen\n0  1997-02-06    1717.2     1779.8     1717.2      1779.8  4435886.0\n1  1997-02-07    1778.2     1808.7     1778.2      1799.0  5448243.0\n2  1997-02-10    1797.4     1797.4     1780.3      1783.3  6513315.0\n3  1997-02-11    1803.3     1832.2     1803.3      1832.0  5146340.0\n4  1997-02-12    1824.6     1824.6     1804.9      1810.2  5748398.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Data</th>\n      <th>Otwarcie</th>\n      <th>Najwyzszy</th>\n      <th>Najnizszy</th>\n      <th>Zamkniecie</th>\n      <th>Wolumen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1997-02-06</td>\n      <td>1717.2</td>\n      <td>1779.8</td>\n      <td>1717.2</td>\n      <td>1779.8</td>\n      <td>4435886.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1997-02-07</td>\n      <td>1778.2</td>\n      <td>1808.7</td>\n      <td>1778.2</td>\n      <td>1799.0</td>\n      <td>5448243.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1997-02-10</td>\n      <td>1797.4</td>\n      <td>1797.4</td>\n      <td>1780.3</td>\n      <td>1783.3</td>\n      <td>6513315.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1997-02-11</td>\n      <td>1803.3</td>\n      <td>1832.2</td>\n      <td>1803.3</td>\n      <td>1832.0</td>\n      <td>5146340.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1997-02-12</td>\n      <td>1824.6</td>\n      <td>1824.6</td>\n      <td>1804.9</td>\n      <td>1810.2</td>\n      <td>5748398.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.read_csv(\"Data/wig20_d.csv\")\n",
    "# df = pd.read_csv(\"Data/mwig40_d.csv\")\n",
    "# df = pd.read_csv(\"Data/swig80_d.csv\")\n",
    "\n",
    "_df.drop(range(1000), inplace=True)\n",
    "_df.reset_index(inplace=True, drop=True)\n",
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5, 6429)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _df.drop(columns=[\"Data\"]).values.transpose()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_fall, thr_rise = (-0.005270574305918364, 0.004968199728502841)\n",
    "\n",
    "data_pipeline = DataProcess(data, test_ratio=0.2, validation_ratio=0.2, batch_size=32, threshold_fall=thr_fall, threshold_rise=thr_rise, feature_to_predict_num=3)\n",
    "data_pipeline.run()\n",
    "_x_train, _y_train, _x_validation, _y_validation, _x_test, _y_test = data_pipeline.get_data()\n",
    "_y_test_ind, _y_validation_ind = np.argmax(_y_test, axis=1), np.argmax(_y_validation, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                           x          y\ntrain       (3824, 5, 32, 2)  (3824, 3)\ntest        (1254, 5, 32, 2)  (1254, 3)\nvalidation  (1254, 5, 32, 2)  (1254, 3)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>(3824, 5, 32, 2)</td>\n      <td>(3824, 3)</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>(1254, 5, 32, 2)</td>\n      <td>(1254, 3)</td>\n    </tr>\n    <tr>\n      <th>validation</th>\n      <td>(1254, 5, 32, 2)</td>\n      <td>(1254, 3)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        \"x\": [_x_train.shape, _x_test.shape, _x_validation.shape],\n",
    "        \"y\": [_y_train.shape, _y_test.shape, _y_validation.shape]\n",
    "    },\n",
    "    index = [\"train\", \"test\", \"validation\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset classes value counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([1330, 1112, 1382])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train dataset classes value counts:\")\n",
    "_y_train.sum(axis=0)\n",
    "# pd.DataFrame(\n",
    "#     {\n",
    "#         str(key): val for (key, val) in zip(*np.unique(np.argmax(_y_train, axis=1), return_counts=True))\n",
    "#     },\n",
    "#     index = [\"count\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:12:21.237913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-14 18:12:21.238294: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-14 18:12:21.238321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (banjaro): /proc/driver/nvidia/version does not exist\n",
      "2023-03-14 18:12:21.239386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape=_x_train.shape[1:]))\n",
    "for i in range(7):\n",
    "    model.add(tf.keras.layers.Conv2D(2, (1, 3), 1, padding=\"same\"))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(1, 3), strides=(1, 3), padding='same'))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(12))\n",
    "model.add(tf.keras.layers.Dense(7))\n",
    "model.add(tf.keras.layers.Dense(3, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=_x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 5, 32, 2)          14        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 5, 11, 2)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 5, 11, 2)          14        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 4, 2)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 4, 2)           14        \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 2, 2)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 5, 2, 2)           14        \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 1, 2)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 5, 1, 2)           14        \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 5, 1, 2)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 5, 1, 2)           14        \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 5, 1, 2)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 5, 1, 2)           14        \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 5, 1, 2)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                132       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 91        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 24        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 345\n",
      "Trainable params: 345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=f1_m\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 31/120 [======>.......................] - ETA: 0s - loss: 25876.8691 - f1_m: 0.4183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 18:12:30.310146: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 18148440 exceeds 10% of free system memory.\n",
      "2023-03-14 18:12:30.310171: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 18148440 exceeds 10% of free system memory.\n",
      "2023-03-14 18:12:30.310291: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 18148440 exceeds 10% of free system memory.\n",
      "2023-03-14 18:12:30.310309: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 18148440 exceeds 10% of free system memory.\n",
      "2023-03-14 18:12:30.310395: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 18148440 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 8ms/step - loss: 24233.4453 - f1_m: 0.3906 - val_loss: 24597.7441 - val_f1_m: 0.5518\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 15673.9902 - f1_m: 0.3896 - val_loss: 15482.9111 - val_f1_m: 0.4740\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 11514.1572 - f1_m: 0.3650 - val_loss: 13771.8818 - val_f1_m: 0.4497\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 10140.7119 - f1_m: 0.3644 - val_loss: 11999.2744 - val_f1_m: 0.4669\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 8837.3105 - f1_m: 0.3657 - val_loss: 10364.8311 - val_f1_m: 0.4701\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 7586.6514 - f1_m: 0.3632 - val_loss: 8819.8457 - val_f1_m: 0.4622\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 6386.8794 - f1_m: 0.3642 - val_loss: 7329.3857 - val_f1_m: 0.4482\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 5214.7964 - f1_m: 0.3644 - val_loss: 5858.1445 - val_f1_m: 0.4372\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4072.7434 - f1_m: 0.3662 - val_loss: 4317.5928 - val_f1_m: 0.4693\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2963.1641 - f1_m: 0.3667 - val_loss: 2953.8657 - val_f1_m: 0.4466\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 1882.7513 - f1_m: 0.3628 - val_loss: 1612.8512 - val_f1_m: 0.4615\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 1115.6293 - f1_m: 0.3558 - val_loss: 1305.6019 - val_f1_m: 0.3784\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 1020.2390 - f1_m: 0.3527 - val_loss: 1184.4878 - val_f1_m: 0.3904\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 948.4424 - f1_m: 0.3589 - val_loss: 1161.9574 - val_f1_m: 0.3768\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 881.2428 - f1_m: 0.3519 - val_loss: 1047.6178 - val_f1_m: 0.3807\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 815.9210 - f1_m: 0.3538 - val_loss: 936.4386 - val_f1_m: 0.3841\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 751.7806 - f1_m: 0.3574 - val_loss: 848.6456 - val_f1_m: 0.4029\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 693.5673 - f1_m: 0.3534 - val_loss: 872.0170 - val_f1_m: 0.3635\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 640.3084 - f1_m: 0.3475 - val_loss: 758.4557 - val_f1_m: 0.3753\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 576.0159 - f1_m: 0.3524 - val_loss: 668.0713 - val_f1_m: 0.3763\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 521.9376 - f1_m: 0.3525 - val_loss: 581.5464 - val_f1_m: 0.3927\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 467.5001 - f1_m: 0.3568 - val_loss: 539.1857 - val_f1_m: 0.3721\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 416.2781 - f1_m: 0.3493 - val_loss: 465.3311 - val_f1_m: 0.4036\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 365.6461 - f1_m: 0.3480 - val_loss: 408.6799 - val_f1_m: 0.3763\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 314.2083 - f1_m: 0.3514 - val_loss: 341.9768 - val_f1_m: 0.3747\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 269.0680 - f1_m: 0.3511 - val_loss: 311.2040 - val_f1_m: 0.3667\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 221.6961 - f1_m: 0.3574 - val_loss: 232.0669 - val_f1_m: 0.4099\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 169.6885 - f1_m: 0.3596 - val_loss: 192.5297 - val_f1_m: 0.3448\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 121.6303 - f1_m: 0.3573 - val_loss: 181.8195 - val_f1_m: 0.3214\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 87.7649 - f1_m: 0.3564 - val_loss: 73.0422 - val_f1_m: 0.3893\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 40.3013 - f1_m: 0.3522 - val_loss: 20.3015 - val_f1_m: 0.3667\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 22.4706 - f1_m: 0.3530 - val_loss: 23.0467 - val_f1_m: 0.2284\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 19.8355 - f1_m: 0.3482 - val_loss: 77.5194 - val_f1_m: 0.2354\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 15.2995 - f1_m: 0.3647 - val_loss: 11.1218 - val_f1_m: 0.4559\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 20.3589 - f1_m: 0.3599 - val_loss: 26.1403 - val_f1_m: 0.2385\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 12.7329 - f1_m: 0.3653 - val_loss: 39.1628 - val_f1_m: 0.2378\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 19.1207 - f1_m: 0.3659 - val_loss: 8.3553 - val_f1_m: 0.2612\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 11.9836 - f1_m: 0.3459 - val_loss: 14.1409 - val_f1_m: 0.2719\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 14.5769 - f1_m: 0.3665 - val_loss: 53.9516 - val_f1_m: 0.2307\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 13.4287 - f1_m: 0.3586 - val_loss: 5.8521 - val_f1_m: 0.5371\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 14.1820 - f1_m: 0.3630 - val_loss: 62.0377 - val_f1_m: 0.2323\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.6496 - f1_m: 0.3562 - val_loss: 21.9599 - val_f1_m: 0.2940\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 11.2137 - f1_m: 0.3569 - val_loss: 4.8123 - val_f1_m: 0.2459\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 10.7122 - f1_m: 0.3599 - val_loss: 4.8251 - val_f1_m: 0.4013\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.8891 - f1_m: 0.3636 - val_loss: 9.3484 - val_f1_m: 0.3401\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.6909 - f1_m: 0.3674 - val_loss: 29.0831 - val_f1_m: 0.2276\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 15.7132 - f1_m: 0.3525 - val_loss: 17.1649 - val_f1_m: 0.5479\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 14.1835 - f1_m: 0.3516 - val_loss: 29.2856 - val_f1_m: 0.5471\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 14.5292 - f1_m: 0.3452 - val_loss: 28.0346 - val_f1_m: 0.2370\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.8078 - f1_m: 0.3422 - val_loss: 28.8006 - val_f1_m: 0.5479\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.2404 - f1_m: 0.3622 - val_loss: 15.7808 - val_f1_m: 0.5464\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 8.3113 - f1_m: 0.3577 - val_loss: 8.3450 - val_f1_m: 0.2395\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 14.6690 - f1_m: 0.3481 - val_loss: 33.6690 - val_f1_m: 0.2307\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 12.8821 - f1_m: 0.3558 - val_loss: 33.6240 - val_f1_m: 0.2363\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 17.7229 - f1_m: 0.3615 - val_loss: 22.5516 - val_f1_m: 0.2294\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 12.1788 - f1_m: 0.3518 - val_loss: 43.4657 - val_f1_m: 0.2307\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 13.9667 - f1_m: 0.3457 - val_loss: 37.6467 - val_f1_m: 0.2307\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 12.0975 - f1_m: 0.3462 - val_loss: 22.1876 - val_f1_m: 0.5487\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 15.4400 - f1_m: 0.3398 - val_loss: 17.1683 - val_f1_m: 0.2307\n",
      "Epoch 60/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 8.2260 - f1_m: 0.3553 - val_loss: 52.6929 - val_f1_m: 0.2307\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 14.6846 - f1_m: 0.3330 - val_loss: 8.2404 - val_f1_m: 0.2409\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.1629 - f1_m: 0.3432 - val_loss: 18.6686 - val_f1_m: 0.2495\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 10.2441 - f1_m: 0.3360 - val_loss: 17.6514 - val_f1_m: 0.2393\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 12.4694 - f1_m: 0.3442 - val_loss: 17.2105 - val_f1_m: 0.2440\n",
      "Epoch 65/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 7.9758 - f1_m: 0.3388 - val_loss: 6.5729 - val_f1_m: 0.2431\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 16.9885 - f1_m: 0.3473 - val_loss: 20.0264 - val_f1_m: 0.2276\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 12.3298 - f1_m: 0.3392 - val_loss: 5.7085 - val_f1_m: 0.5456\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 12.3890 - f1_m: 0.3412 - val_loss: 16.0098 - val_f1_m: 0.2319\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 7.5442 - f1_m: 0.3488 - val_loss: 16.8806 - val_f1_m: 0.2424\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 11.2110 - f1_m: 0.3438 - val_loss: 14.7784 - val_f1_m: 0.5487\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 12.8652 - f1_m: 0.3471 - val_loss: 46.0689 - val_f1_m: 0.2316\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 8.0179 - f1_m: 0.3483 - val_loss: 4.7148 - val_f1_m: 0.4392\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.0737 - f1_m: 0.3556 - val_loss: 18.9858 - val_f1_m: 0.2950\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.6453 - f1_m: 0.3520 - val_loss: 4.1838 - val_f1_m: 0.4650\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 12.5968 - f1_m: 0.3495 - val_loss: 25.2259 - val_f1_m: 0.2387\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.0974 - f1_m: 0.3506 - val_loss: 3.9382 - val_f1_m: 0.4646\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.8683 - f1_m: 0.3325 - val_loss: 25.0890 - val_f1_m: 0.2331\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 7.7240 - f1_m: 0.3435 - val_loss: 31.6196 - val_f1_m: 0.2331\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 8.7917 - f1_m: 0.3584 - val_loss: 19.8894 - val_f1_m: 0.2331\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.3622 - f1_m: 0.3395 - val_loss: 6.0769 - val_f1_m: 0.2288\n",
      "Epoch 81/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.1575 - f1_m: 0.3461 - val_loss: 15.0977 - val_f1_m: 0.2385\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 12.4203 - f1_m: 0.3427 - val_loss: 27.6990 - val_f1_m: 0.2331\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 6.2184 - f1_m: 0.3381 - val_loss: 11.6658 - val_f1_m: 0.2370\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.4496 - f1_m: 0.3429 - val_loss: 3.6951 - val_f1_m: 0.2247\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 10.1960 - f1_m: 0.3479 - val_loss: 10.9025 - val_f1_m: 0.2379\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 13.0891 - f1_m: 0.3353 - val_loss: 39.1389 - val_f1_m: 0.2362\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.3789 - f1_m: 0.3448 - val_loss: 11.2685 - val_f1_m: 0.2428\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 8.8914 - f1_m: 0.3482 - val_loss: 11.0341 - val_f1_m: 0.2612\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 9.7997 - f1_m: 0.3438 - val_loss: 2.7442 - val_f1_m: 0.5604\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 8.0073 - f1_m: 0.3344 - val_loss: 3.6524 - val_f1_m: 0.5568\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 6.5443 - f1_m: 0.3339 - val_loss: 18.3043 - val_f1_m: 0.2339\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 8.9984 - f1_m: 0.3483 - val_loss: 19.4503 - val_f1_m: 0.2395\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 8.4887 - f1_m: 0.3362 - val_loss: 33.1493 - val_f1_m: 0.2346\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 11.9097 - f1_m: 0.3307 - val_loss: 28.2314 - val_f1_m: 0.2331\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 12.9975 - f1_m: 0.3254 - val_loss: 19.6988 - val_f1_m: 0.2426\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 8.7902 - f1_m: 0.3438 - val_loss: 10.9442 - val_f1_m: 0.2394\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 8.4416 - f1_m: 0.3375 - val_loss: 4.2888 - val_f1_m: 0.2448\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 8.7729 - f1_m: 0.3312 - val_loss: 6.0035 - val_f1_m: 0.5520\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.0999 - f1_m: 0.3369 - val_loss: 17.7808 - val_f1_m: 0.5503\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 9.0289 - f1_m: 0.3363 - val_loss: 14.2446 - val_f1_m: 0.2304\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 9.0931 - f1_m: 0.3329 - val_loss: 6.7500 - val_f1_m: 0.2415\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 7.1339 - f1_m: 0.3306 - val_loss: 11.5588 - val_f1_m: 0.2383\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 9.8159 - f1_m: 0.3244 - val_loss: 19.7846 - val_f1_m: 0.5180\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.3179 - f1_m: 0.3416 - val_loss: 10.1897 - val_f1_m: 0.5518\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 5.7779 - f1_m: 0.3513 - val_loss: 5.3661 - val_f1_m: 0.2314\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 7.4841 - f1_m: 0.3302 - val_loss: 2.0542 - val_f1_m: 0.3367\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 7.0301 - f1_m: 0.3292 - val_loss: 11.3455 - val_f1_m: 0.2337\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.1389 - f1_m: 0.3339 - val_loss: 14.9536 - val_f1_m: 0.2378\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 11.2332 - f1_m: 0.3408 - val_loss: 18.3291 - val_f1_m: 0.2226\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 6.7165 - f1_m: 0.3393 - val_loss: 11.9002 - val_f1_m: 0.5440\n",
      "Epoch 111/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 10.9055 - f1_m: 0.3364 - val_loss: 4.6959 - val_f1_m: 0.3802\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.4307 - f1_m: 0.3277 - val_loss: 37.1469 - val_f1_m: 0.2217\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 7.1438 - f1_m: 0.3359 - val_loss: 11.5423 - val_f1_m: 0.5407\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 7.1536 - f1_m: 0.3395 - val_loss: 5.1145 - val_f1_m: 0.3198\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.2905 - f1_m: 0.3549 - val_loss: 7.3468 - val_f1_m: 0.2238\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.5576 - f1_m: 0.3316 - val_loss: 8.3662 - val_f1_m: 0.2408\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 7.6373 - f1_m: 0.3325 - val_loss: 22.5475 - val_f1_m: 0.3003\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 6.6116 - f1_m: 0.3313 - val_loss: 9.1928 - val_f1_m: 0.5434\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 9.2883 - f1_m: 0.3398 - val_loss: 14.8593 - val_f1_m: 0.2481\n",
      "Epoch 120/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 6.6067 - f1_m: 0.3343 - val_loss: 2.5834 - val_f1_m: 0.2902\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.9647 - f1_m: 0.3240 - val_loss: 2.9975 - val_f1_m: 0.2285\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 5.5642 - f1_m: 0.3260 - val_loss: 5.6404 - val_f1_m: 0.2469\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.2693 - f1_m: 0.3266 - val_loss: 2.2581 - val_f1_m: 0.1973\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.6062 - f1_m: 0.3355 - val_loss: 3.7374 - val_f1_m: 0.3881\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 5.2488 - f1_m: 0.3300 - val_loss: 5.1152 - val_f1_m: 0.3573\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.1780 - f1_m: 0.3054 - val_loss: 9.9115 - val_f1_m: 0.2257\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 7.0452 - f1_m: 0.3197 - val_loss: 9.2082 - val_f1_m: 0.2591\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 3.2129 - f1_m: 0.3164 - val_loss: 5.1484 - val_f1_m: 0.2330\n",
      "Epoch 129/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.6304 - f1_m: 0.3304 - val_loss: 4.2900 - val_f1_m: 0.2201\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.9714 - f1_m: 0.3109 - val_loss: 2.9589 - val_f1_m: 0.2663\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.9474 - f1_m: 0.3130 - val_loss: 8.1187 - val_f1_m: 0.5279\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 6.4863 - f1_m: 0.3326 - val_loss: 8.0440 - val_f1_m: 0.2571\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 5.3735 - f1_m: 0.3067 - val_loss: 3.3159 - val_f1_m: 0.2395\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 4.8779 - f1_m: 0.3256 - val_loss: 8.9146 - val_f1_m: 0.2556\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.5168 - f1_m: 0.3309 - val_loss: 2.5497 - val_f1_m: 0.5518\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.4022 - f1_m: 0.2937 - val_loss: 7.0833 - val_f1_m: 0.2438\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.0967 - f1_m: 0.3183 - val_loss: 2.0734 - val_f1_m: 0.3429\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.6219 - f1_m: 0.2717 - val_loss: 3.4167 - val_f1_m: 0.5068\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.8693 - f1_m: 0.3028 - val_loss: 5.6011 - val_f1_m: 0.5088\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.1273 - f1_m: 0.3046 - val_loss: 2.3161 - val_f1_m: 0.3314\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.0177 - f1_m: 0.2928 - val_loss: 8.1511 - val_f1_m: 0.3010\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 5.8026 - f1_m: 0.3273 - val_loss: 5.3268 - val_f1_m: 0.2351\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.4710 - f1_m: 0.2710 - val_loss: 3.3667 - val_f1_m: 0.4918\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.1246 - f1_m: 0.2982 - val_loss: 4.1518 - val_f1_m: 0.3345\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.6479 - f1_m: 0.3111 - val_loss: 3.2370 - val_f1_m: 0.4602\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.9215 - f1_m: 0.3159 - val_loss: 9.3750 - val_f1_m: 0.2902\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.8505 - f1_m: 0.2970 - val_loss: 2.9223 - val_f1_m: 0.3297\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.2758 - f1_m: 0.2891 - val_loss: 4.8356 - val_f1_m: 0.2512\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.6279 - f1_m: 0.2753 - val_loss: 6.9423 - val_f1_m: 0.2226\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.0621 - f1_m: 0.3051 - val_loss: 8.1782 - val_f1_m: 0.4913\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.6590 - f1_m: 0.3048 - val_loss: 10.8468 - val_f1_m: 0.1997\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.3415 - f1_m: 0.2778 - val_loss: 11.9164 - val_f1_m: 0.2300\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.6808 - f1_m: 0.2888 - val_loss: 2.2483 - val_f1_m: 0.2351\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.3936 - f1_m: 0.2897 - val_loss: 8.6911 - val_f1_m: 0.2824\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.3494 - f1_m: 0.3143 - val_loss: 6.1775 - val_f1_m: 0.3693\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.9778 - f1_m: 0.2911 - val_loss: 1.4129 - val_f1_m: 0.1784\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.7926 - f1_m: 0.2815 - val_loss: 2.7566 - val_f1_m: 0.2254\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.5760 - f1_m: 0.2854 - val_loss: 5.2137 - val_f1_m: 0.2633\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.1157 - f1_m: 0.2856 - val_loss: 8.2853 - val_f1_m: 0.2144\n",
      "Epoch 160/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.5183 - f1_m: 0.2657 - val_loss: 2.3174 - val_f1_m: 0.1849\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.5978 - f1_m: 0.2547 - val_loss: 1.5338 - val_f1_m: 0.2184\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.2648 - f1_m: 0.3004 - val_loss: 5.2836 - val_f1_m: 0.2035\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.5609 - f1_m: 0.2770 - val_loss: 5.6028 - val_f1_m: 0.2949\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.7757 - f1_m: 0.2805 - val_loss: 5.7293 - val_f1_m: 0.4594\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.0275 - f1_m: 0.2710 - val_loss: 9.5594 - val_f1_m: 0.4826\n",
      "Epoch 166/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.2247 - f1_m: 0.2453 - val_loss: 2.6680 - val_f1_m: 0.2630\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.6729 - f1_m: 0.2758 - val_loss: 3.2271 - val_f1_m: 0.3192\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.7127 - f1_m: 0.3002 - val_loss: 4.5399 - val_f1_m: 0.4427\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.9020 - f1_m: 0.2803 - val_loss: 2.5868 - val_f1_m: 0.3288\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.2373 - f1_m: 0.2828 - val_loss: 9.8691 - val_f1_m: 0.3213\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.6659 - f1_m: 0.2830 - val_loss: 2.6722 - val_f1_m: 0.1823\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.1636 - f1_m: 0.2607 - val_loss: 1.4840 - val_f1_m: 0.4748\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.7868 - f1_m: 0.2622 - val_loss: 3.1477 - val_f1_m: 0.2624\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.7447 - f1_m: 0.2654 - val_loss: 7.4347 - val_f1_m: 0.3274\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.7581 - f1_m: 0.2791 - val_loss: 4.9927 - val_f1_m: 0.3306\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 4.6605 - f1_m: 0.2959 - val_loss: 5.8625 - val_f1_m: 0.4404\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.8760 - f1_m: 0.2768 - val_loss: 2.4075 - val_f1_m: 0.1998\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4641 - f1_m: 0.2550 - val_loss: 4.7337 - val_f1_m: 0.2086\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.3216 - f1_m: 0.2884 - val_loss: 7.2220 - val_f1_m: 0.4184\n",
      "Epoch 180/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 4.1918 - f1_m: 0.2961 - val_loss: 4.7294 - val_f1_m: 0.4525\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.5503 - f1_m: 0.2617 - val_loss: 10.2347 - val_f1_m: 0.3250\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.9168 - f1_m: 0.2706 - val_loss: 2.7395 - val_f1_m: 0.4442\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.6369 - f1_m: 0.2763 - val_loss: 8.0366 - val_f1_m: 0.3361\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.2197 - f1_m: 0.2581 - val_loss: 3.4874 - val_f1_m: 0.3097\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4097 - f1_m: 0.2586 - val_loss: 4.4519 - val_f1_m: 0.3407\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.6057 - f1_m: 0.2436 - val_loss: 2.6252 - val_f1_m: 0.4383\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 3.0392 - f1_m: 0.2785 - val_loss: 2.5100 - val_f1_m: 0.3149\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.0497 - f1_m: 0.2491 - val_loss: 3.1144 - val_f1_m: 0.4494\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.2892 - f1_m: 0.2370 - val_loss: 3.2753 - val_f1_m: 0.3523\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.2289 - f1_m: 0.2732 - val_loss: 3.4760 - val_f1_m: 0.4410\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.9935 - f1_m: 0.2360 - val_loss: 2.6749 - val_f1_m: 0.3779\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.5648 - f1_m: 0.2731 - val_loss: 1.4452 - val_f1_m: 0.5043\n",
      "Epoch 193/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.4636 - f1_m: 0.2488 - val_loss: 4.7106 - val_f1_m: 0.2945\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.4280 - f1_m: 0.2642 - val_loss: 2.9702 - val_f1_m: 0.3201\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.1197 - f1_m: 0.2410 - val_loss: 1.2547 - val_f1_m: 0.0959\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.4818 - f1_m: 0.2551 - val_loss: 5.7647 - val_f1_m: 0.3361\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.3489 - f1_m: 0.2552 - val_loss: 4.6335 - val_f1_m: 0.2788\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 3.0269 - f1_m: 0.2656 - val_loss: 4.3792 - val_f1_m: 0.2029\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.5162 - f1_m: 0.2523 - val_loss: 2.0890 - val_f1_m: 0.2949\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.2733 - f1_m: 0.2418 - val_loss: 1.1997 - val_f1_m: 0.2596\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.7200 - f1_m: 0.2153 - val_loss: 1.3431 - val_f1_m: 0.3581\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.3724 - f1_m: 0.2575 - val_loss: 1.6951 - val_f1_m: 0.3513\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.3645 - f1_m: 0.2632 - val_loss: 5.4460 - val_f1_m: 0.3685\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.7239 - f1_m: 0.2701 - val_loss: 2.7015 - val_f1_m: 0.4435\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.4082 - f1_m: 0.2635 - val_loss: 1.9344 - val_f1_m: 0.3309\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.2188 - f1_m: 0.2454 - val_loss: 2.5715 - val_f1_m: 0.4448\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4754 - f1_m: 0.2500 - val_loss: 1.7115 - val_f1_m: 0.3994\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4826 - f1_m: 0.2480 - val_loss: 1.5566 - val_f1_m: 0.2608\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.0381 - f1_m: 0.2222 - val_loss: 2.3467 - val_f1_m: 0.4164\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.5713 - f1_m: 0.2669 - val_loss: 1.8191 - val_f1_m: 0.4393\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.7043 - f1_m: 0.2526 - val_loss: 5.6901 - val_f1_m: 0.3344\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.9624 - f1_m: 0.2744 - val_loss: 1.7055 - val_f1_m: 0.3919\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.2031 - f1_m: 0.2447 - val_loss: 3.9634 - val_f1_m: 0.4468\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 1.9002 - f1_m: 0.2167 - val_loss: 3.5341 - val_f1_m: 0.3342\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.0066 - f1_m: 0.2145 - val_loss: 2.7371 - val_f1_m: 0.3202\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.2928 - f1_m: 0.2369 - val_loss: 1.5306 - val_f1_m: 0.3710\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.6917 - f1_m: 0.2598 - val_loss: 1.4379 - val_f1_m: 0.3902\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 1.7838 - f1_m: 0.2163 - val_loss: 2.7275 - val_f1_m: 0.1932\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.0150 - f1_m: 0.2298 - val_loss: 1.3136 - val_f1_m: 0.1832\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.3673 - f1_m: 0.2467 - val_loss: 2.6022 - val_f1_m: 0.1908\n",
      "Epoch 221/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.3919 - f1_m: 0.2430 - val_loss: 2.1394 - val_f1_m: 0.3816\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4157 - f1_m: 0.2487 - val_loss: 2.5356 - val_f1_m: 0.2038\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4927 - f1_m: 0.2374 - val_loss: 2.9935 - val_f1_m: 0.4393\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.7069 - f1_m: 0.2576 - val_loss: 2.9128 - val_f1_m: 0.3695\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.0978 - f1_m: 0.2342 - val_loss: 1.6124 - val_f1_m: 0.3582\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.3065 - f1_m: 0.2408 - val_loss: 1.2995 - val_f1_m: 0.2245\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.9901 - f1_m: 0.2219 - val_loss: 2.8228 - val_f1_m: 0.3307\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.4036 - f1_m: 0.2385 - val_loss: 2.0053 - val_f1_m: 0.1715\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 2.4916 - f1_m: 0.2596 - val_loss: 2.4559 - val_f1_m: 0.3831\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 1.7263 - f1_m: 0.2172 - val_loss: 2.3684 - val_f1_m: 0.3288\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.3940 - f1_m: 0.2456 - val_loss: 1.7737 - val_f1_m: 0.4174\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 2.5886 - f1_m: 0.2704 - val_loss: 2.7370 - val_f1_m: 0.4071\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 1.8535 - f1_m: 0.2073 - val_loss: 1.4619 - val_f1_m: 0.3167\n"
     ]
    }
   ],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m',\n",
    "                                      mode='max',\n",
    "                                      min_delta=1e-3,\n",
    "                                      patience=20,\n",
    "                                      start_from_epoch=200,\n",
    "                                      )\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('Models/cdt_2/best_model.h5',\n",
    "                                        monitor='val_f1_m',\n",
    "                                        mode='max'\n",
    "                                        )\n",
    "\n",
    "history = model.fit(_x_train, _y_train,\n",
    "                    epochs=500,\n",
    "                    callbacks=[es, mc],\n",
    "                    validation_data=(_x_test, _y_test)\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"Models/cdt_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(_x_test, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(_x_test)\n",
    "pred_test_ind = np.argmax(pred_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test correct:\", np.sum(pred_test_ind == _y_test_ind))\n",
    "print(\"Test accuracy:\", np.sum(pred_test_ind == _y_test_ind)/pred_test_ind.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(_y_test_ind, pred_test_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(_x_validation, _y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation = model.predict(_x_validation)\n",
    "pred_validation_ind = np.argmax(pred_validation, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation correct:\", np.sum(pred_validation_ind == _y_validation_ind))\n",
    "print(\"Validation accuracy:\", np.sum(pred_validation_ind == _y_validation_ind)/pred_validation_ind.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(_y_validation_ind, pred_validation_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Draft"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inzynierka",
   "language": "python",
   "name": "inzynierka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
